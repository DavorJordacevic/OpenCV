{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Detection with OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first example weâ€™ll learn how to apply face detection with OpenCV to the video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to import some required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN stands for OpenCV: Deep Neural Networks\n",
    "DNN = \"TF\" # Or CAFFE, or any other suported framework\n",
    "min_confidence = 0.5 # minimum probability to filter weak detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "These files can be downloaded from the Internet, or created and trained manually.\n",
    "\n",
    "For Caffe:\n",
    "\n",
    "* res10_300x300_ssd_iter_140000_fp16.caffemodel\n",
    "\n",
    "* deploy.prototxt\n",
    "\n",
    "For Tensorflow:\n",
    "\n",
    "* opencv_face_detector_uint8.pb\n",
    "\n",
    "* opencv_face_detector.pbtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading model...\n",
      "[INFO] model loaded.\n"
     ]
    }
   ],
   "source": [
    "# load our serialized model from disk\n",
    "print(\"[INFO] loading model...\")\n",
    "\n",
    "if DNN == \"CAFFE\":\n",
    "    modelFile = \"res10_300x300_ssd_iter_140000_fp16.caffemodel\"\n",
    "    configFile= \"deploy.prototxt\"\n",
    "    \n",
    "    # Here we need to read our pre-trained neural net created using Caffe\n",
    "    net = cv2.dnn.readNetFromCaffe(configFile, modelFile)\n",
    "else:\n",
    "    modelFile = \"opencv_face_detector_uint8.pb\"\n",
    "    configFile= \"opencv_face_detector.pbtxt\"\n",
    "    \n",
    "    # Here we need to read our pre-trained neural net created using Tensorflow\n",
    "    net = cv2.dnn.readNetFromTensorflow(modelFile, configFile)\n",
    "    \n",
    "print(\"[INFO] model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**cv2.dnn.blobFromImage**\n",
    "\n",
    "This function perform:\n",
    "\n",
    "* Mean subtraction\n",
    "* Scaling\n",
    "* Channel swapping (optionally)\n",
    "\n",
    "**Mean subtraction** is used to help combat illumination changes in the input images in our dataset.\n",
    "\n",
    "Before we even begin training our deep neural network, we first compute the average pixel intensity across all images in the training set for each of the Red, Green, and Blue channels.\n",
    "\n",
    "This implies that we end up with three variables:\n",
    "\n",
    "$\\mu_R$, $\\mu_G$, and $\\mu_B$\n",
    "\n",
    "Typically the resulting values are a 3-tuple consisting of the mean of the Red, Green, and Blue channels, respectively.\n",
    "\n",
    "When we are ready to pass an image through our network (whether for training or testing), we subtract the mean, \\mu, from each input channel of the input image:\n",
    "\n",
    "R = R - $\\mu_R$\n",
    "\n",
    "G = G - $\\mu_G$\n",
    "\n",
    "B = B - $\\mu_B$\n",
    "\n",
    "We may also have a scaling factor, $\\sigma$. The value of $\\sigma$ may be the standard deviation across the training set which adds in a normalization:\n",
    "\n",
    "R = (R - $\\mu_R$) / $\\sigma$\n",
    "\n",
    "G = (G - $\\mu_G$) / $\\sigma$\n",
    "\n",
    "B = (B - $\\mu_B$) / $\\sigma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function signature:\n",
    "\n",
    "    blob = cv2.dnn.blobFromImage(image, scalefactor=1.0, size, mean, swapRB=True)\n",
    "    \n",
    "Where:\n",
    "* scalefactor  - we can optionally scale our images by some factor. This value defaults to 1.0 (no scaling) \n",
    "* size - spatial size that the Convolutional Neural Network expects\n",
    "* mean - our mean subtraction values\n",
    "* swapRB -  OpenCV assumes images are in BGR channel order; however, the mean value assumes we are using RGB order. To resolve this discrepancy we can swap the R and B channels in image  by setting this value to True."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "cv2::dnn::Net Class Reference\n",
    "This class allows to create and manipulate comprehensive artificial neural networks.\n",
    "\n",
    "Neural network is presented as directed acyclic graph (DAG), where vertices are Layer instances, and edges specify relationships between layers inputs and outputs.\n",
    "\n",
    "Each network layer has unique integer id and unique string name inside its network. LayerId can store either layer name or layer id.\n",
    "\n",
    "This class supports reference counting of its instances, i. e. copies point to the same instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the input video and construct an input blob for every frame\n",
    "# by resizing to a fixed 600x400 pixels and then normalizing it\n",
    "\n",
    "cap = cv2.VideoCapture(\"babies-video2.mp4\")\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Our operations on the frame come here\n",
    "    frame1 = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    frame1 = cv2.resize(frame,(int(600),int(400)))\n",
    "\n",
    "    blob = cv2.dnn.blobFromImage(cv2.resize(frame1, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "    \n",
    "    (h, w) = frame1.shape[:2]\n",
    "    # loop over the detections\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        # extract the confidence (probability) associated with the prediction\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "\n",
    "        # filter out weak detections by ensuring the `confidence` is\n",
    "        # greater than the minimum confidence\n",
    "        if confidence > min_confidence:\n",
    "            # compute the (x, y)-coordinates of the bounding box for the\n",
    "            # object\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "            # draw the bounding box of the face\n",
    "            cv2.rectangle(frame1, (startX, startY), (endX, endY),(0, 69, 255), 2)\n",
    "\n",
    "    # show the output frame\n",
    "    cv2.imshow(\"Frame\", frame1)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    " \n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "        \n",
    "# do a bit of cleanup\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
